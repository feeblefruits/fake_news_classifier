{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/feeblefruits/disaster_responseline_text_classifier/blob/master/models/train_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import string\n",
    "import re\n",
    "import bs4 as BeautifulSoup\n",
    "import fasttext\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jacques/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jacques/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jacques/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class CaseCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        n_title_case = pd.Series(X).apply(lambda x: sum(1 for c in x if c.isupper())).values\n",
    "        n_char = X.str.len()\n",
    "        \n",
    "        X_values = n_title_case / n_char\n",
    "        \n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class StopWordCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        '''\n",
    "        INPUT: string\n",
    "        OUPUT: number of stopwords found (int)\n",
    "        '''\n",
    "\n",
    "        X_values = []\n",
    "\n",
    "        for text in X:\n",
    "            stop_words_found = []\n",
    "\n",
    "            for i in text.split():\n",
    "                if i in stop_words:\n",
    "                    stop_words_found.append(1)\n",
    "\n",
    "            X_values.append(sum(stop_words_found))\n",
    "\n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class WordPronounCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X_values = []\n",
    "\n",
    "        for text in X:\n",
    "            pronouns = []\n",
    "\n",
    "            for token in nlp(text):\n",
    "                if token.pos_ == 'PRON':\n",
    "                    pronouns.append(token)\n",
    "\n",
    "                X_values.append(len(pronouns))\n",
    "                \n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    '''\n",
    "    INPUT: String to tokenise, detect and replace URLs\n",
    "    OUTPUT: List of tokenised string items\n",
    "    '''\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = [w for w in text.split() if not w in stop_words]\n",
    "\n",
    "    # Join list to string\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Replace URLs if any\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # Setup tokens and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Create tokens and lemmatize\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "\n",
    "    '''\n",
    "    Defining pipeline which includes custom transformer\n",
    "    '''\n",
    "    \n",
    "    # define pipeline    \n",
    "    pipeline = Pipeline([\n",
    "        \n",
    "        # define featureunion inside pipeline\n",
    "        ('features', FeatureUnion\n",
    "         ([\n",
    "             # define another pipeline inside featureunion\n",
    "             ('nlp_pipe', Pipeline\n",
    "              ([\n",
    "                  ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                  ('tfidf', TfidfTransformer())\n",
    "              ]))\n",
    "            # define custom transformer\n",
    "#             ('case_counter', CaseCounter()),\n",
    "#             ('stop_counter', StopWordCounter()),\n",
    "#             ('pronoun_counter', WordPronounCounter())\n",
    "         ])),\n",
    "        # round off with predict transformer\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])    \n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/corona_fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "\n",
    "    '''\n",
    "    INPUT: string directory of db\n",
    "    OUTPUT: x message pd column, y categorical column labels, categorical names\n",
    "    '''\n",
    "    \n",
    "    df = df.dropna(subset=['title'])\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df.dropna(subset=['label'])\n",
    "    \n",
    "    df.drop(axis=1, labels='source', inplace=True)\n",
    "    \n",
    "    df['label'].replace('TRUE', 1, inplace=True)\n",
    "    df['label'].replace('fake', 0, inplace=True)\n",
    "    df['label'].replace('Fake', 0, inplace=True)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # read in file\n",
    "    X = df['title']\n",
    "    Y = df['label']\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacques/opt/anaconda3/envs/data-sci/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[ 93  44]\n",
      " [ 14 117]]\n",
      "Accuracy: 0.7835820895522388\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
