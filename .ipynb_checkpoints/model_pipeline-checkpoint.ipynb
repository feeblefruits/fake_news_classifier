{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import string\n",
    "import re\n",
    "import bs4 as BeautifulSoup\n",
    "import fasttext\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup custom transformers for pipeine\n",
    "\n",
    "Features extracted via custom transformers are based on trends discovered during data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class CharCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        n_char = X.str.len()\n",
    "        \n",
    "        X_values = n_char\n",
    "        \n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class CaseCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        n_title_case = pd.Series(X).apply(lambda x: sum(1 for c in x if c.isupper())).values\n",
    "        n_char = X.str.len()\n",
    "        \n",
    "        X_values = n_title_case / n_char\n",
    "        \n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class StopWordCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        '''\n",
    "        INPUT: string\n",
    "        OUPUT: number of stopwords found (int)\n",
    "        '''\n",
    "\n",
    "        X_values = []\n",
    "\n",
    "        for text in X:\n",
    "            stop_words_found = []\n",
    "\n",
    "            for i in text.split():\n",
    "                if i in stop_words:\n",
    "                    stop_words_found.append(1)\n",
    "\n",
    "            X_values.append(sum(stop_words_found))\n",
    "\n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class WordPronCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X_values = []\n",
    "\n",
    "        for text in X:\n",
    "            pronouns = []\n",
    "\n",
    "            for token in nlp(text):\n",
    "                if token.pos_ == 'PRON':\n",
    "                    pronouns.append(token)\n",
    "\n",
    "            X_values.append(len(pronouns))\n",
    "                \n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class WordNounCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X_values = []\n",
    "\n",
    "        for text in X:\n",
    "            pronouns = []\n",
    "\n",
    "            for token in nlp(text):\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    pronouns.append(token)\n",
    "\n",
    "            X_values.append(len(pronouns))\n",
    "                \n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for custom transformer\n",
    "class WordAdjCounter(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    # takes in a 2d array X for the feature data and a 1d array y for the target labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # transform method also takes a 2d array X\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X_values = []\n",
    "\n",
    "        for text in X:\n",
    "            pronouns = []\n",
    "\n",
    "            for token in nlp(text):\n",
    "                if token.pos_ == 'ADJ':\n",
    "                    pronouns.append(token)\n",
    "\n",
    "            X_values.append(len(pronouns))\n",
    "                \n",
    "        return pd.DataFrame(np.array(X_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    '''\n",
    "    INPUT: String to tokenise, detect and replace URLs\n",
    "    OUTPUT: List of tokenised string items\n",
    "    '''\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = [w for w in text.split() if not w in stop_words]\n",
    "\n",
    "    # Join list to string\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Replace URLs if any\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # Setup tokens and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Create tokens and lemmatize\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('char_counter', CharCounter()),\n",
    "            ('case_counter', CaseCounter()),\n",
    "            ('stop_counter', StopWordCounter()),\n",
    "            ('pro_counter', WordPronCounter()),\n",
    "            ('noun_counter', WordNounCounter()),\n",
    "            ('adj_counter', WordAdjCounter())\n",
    "        ])),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/corona_fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "\n",
    "    '''\n",
    "    INPUT: string directory of db\n",
    "    OUTPUT: x message pd column, y categorical column labels, categorical names\n",
    "    '''\n",
    "    \n",
    "    df = df.dropna(subset=['title'])\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df.dropna(subset=['label'])\n",
    "    \n",
    "    df.drop(axis=1, labels='source', inplace=True)\n",
    "    \n",
    "    df['label'].replace('TRUE', 1, inplace=True)\n",
    "    df['label'].replace('fake', 0, inplace=True)\n",
    "    df['label'].replace('Fake', 0, inplace=True)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # read in file\n",
    "    X = df['title']\n",
    "    Y = df['label']\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8544776119402985\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true=y_test,\n",
    "                               y_pred=y_pred,\n",
    "                               target_names=['fake','true'],\n",
    "                               output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow0_col0 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow0_col1 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }    #T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow0_col2 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow1_col0 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }    #T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow1_col1 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow1_col2 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }</style><table id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcb\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >precision</th>        <th class=\"col_heading level0 col1\" >recall</th>        <th class=\"col_heading level0 col2\" >f1-score</th>        <th class=\"col_heading level0 col3\" >support</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcblevel0_row0\" class=\"row_heading level0 row0\" >fake</th>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow0_col0\" class=\"data row0 col0\" >0.771930</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow0_col1\" class=\"data row0 col1\" >0.871287</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow0_col2\" class=\"data row0 col2\" >0.818605</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow0_col3\" class=\"data row0 col3\" >101.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcblevel0_row1\" class=\"row_heading level0 row1\" >true</th>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow1_col0\" class=\"data row1 col0\" >0.915584</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow1_col1\" class=\"data row1 col1\" >0.844311</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow1_col2\" class=\"data row1 col2\" >0.878505</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow1_col3\" class=\"data row1 col3\" >167.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcblevel0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow2_col0\" class=\"data row2 col0\" >0.854478</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow2_col1\" class=\"data row2 col1\" >0.854478</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow2_col2\" class=\"data row2 col2\" >0.854478</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow2_col3\" class=\"data row2 col3\" >0.854478</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcblevel0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow3_col0\" class=\"data row3 col0\" >0.843757</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow3_col1\" class=\"data row3 col1\" >0.857799</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow3_col2\" class=\"data row3 col2\" >0.848555</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow3_col3\" class=\"data row3 col3\" >268.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcblevel0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow4_col0\" class=\"data row4 col0\" >0.861446</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow4_col1\" class=\"data row4 col1\" >0.854478</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow4_col2\" class=\"data row4 col2\" >0.855930</td>\n",
       "                        <td id=\"T_3ca999e6_0c7d_11eb_bf9e_38f9d3537bcbrow4_col3\" class=\"data row4 col3\" >268.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fec03d0fd90>"
      ]
     },
     "execution_count": 1048,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(report).transpose().style\\\n",
    ".highlight_max(color='lightgreen', subset=['precision'])\\\n",
    ".highlight_min(color='lightgreen', subset=['precision'] )\\\n",
    ".highlight_max(color='lightgreen', subset=['recall'])\\\n",
    ".highlight_min(color='lightgreen', subset=['recall'] )\\\n",
    ".highlight_max(color='lightgreen', subset=['f1-score'])\\\n",
    ".highlight_min(color='lightgreen', subset=['f1-score'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multiple classifer test</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  10.262845675150553\n",
      "fit_time  std  0.16700671216670246\n",
      "score_time  mean  4.970849990844727\n",
      "score_time  std  0.05395628617170741\n",
      "test_score  mean  0.7826431680968872\n",
      "test_score  std  0.04404347289380314\n",
      "---------------------------------\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "-----------------------------------\n",
      "fit_time  mean  9.942005316416422\n",
      "fit_time  std  0.02505407877537073\n",
      "score_time  mean  4.890467643737793\n",
      "score_time  std  0.032168330305232994\n",
      "test_score  mean  0.7777615961195403\n",
      "test_score  std  0.012647713640927639\n",
      "---------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  10.162290016810099\n",
      "fit_time  std  0.22086413677368585\n",
      "score_time  mean  4.865853627522786\n",
      "score_time  std  0.07994436766152443\n",
      "test_score  mean  0.7915087203726797\n",
      "test_score  std  0.008848105476424123\n",
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  10.049817085266113\n",
      "fit_time  std  0.33288538334710205\n",
      "score_time  mean  4.875288327534993\n",
      "score_time  std  0.034587135477707136\n",
      "test_score  mean  0.6753840271762194\n",
      "test_score  std  0.00792383441732688\n",
      "---------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  9.899986982345581\n",
      "fit_time  std  0.30791437514342623\n",
      "score_time  mean  5.160759925842285\n",
      "score_time  std  0.47019631673932394\n",
      "test_score  mean  0.8076871562886198\n",
      "test_score  std  0.019896069785525366\n",
      "---------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "-----------------------------------\n",
      "fit_time  mean  9.812803665796915\n",
      "fit_time  std  0.05022837002059197\n",
      "score_time  mean  4.863463083902995\n",
      "score_time  std  0.05700390897154843\n",
      "test_score  mean  0.6866014538834649\n",
      "test_score  std  0.01928432339958376\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "-----------------------------------\n",
      "fit_time  mean  9.917058308919271\n",
      "fit_time  std  0.11225300513205681\n",
      "score_time  mean  4.897386391957601\n",
      "score_time  std  0.12615628809007212\n",
      "test_score  mean  0.7465458113952779\n",
      "test_score  std  0.015689183764423287\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(AdaBoostClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "clfs.append(SVC())\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "\n",
    "classifier_name = []\n",
    "mean_value = []\n",
    "std_value = []\n",
    "\n",
    "for classifier in clfs:\n",
    "    pipeline.set_params(clf = classifier)\n",
    "    scores = cross_validate(pipeline, X_train, y_train)\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    for key, values in scores.items():\n",
    "        \n",
    "        classifier_name.append(classifier)\n",
    "        mean_value.append(values.mean())\n",
    "        std_value.append(values.std())\n",
    "        \n",
    "        print(key,' mean ', values.mean())\n",
    "        print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('char_counter', CharCounter()),\n",
    "            ('case_counter', CaseCounter()),\n",
    "            ('stop_counter', StopWordCounter()),\n",
    "            ('pro_counter', WordPronCounter()),\n",
    "            ('noun_counter', WordNounCounter()),\n",
    "            ('adj_counter', WordAdjCounter())\n",
    "        ])),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = model_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \n",
    "    'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "    'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "    'clf__fit_intercept': (True, False),\n",
    "    'features__transformer_weights': (\n",
    "        {'text_pipeline': 1, 'char_counter': 0.5},\n",
    "        {'text_pipeline': 0.5, 'char_counter': 1},\n",
    "    )\n",
    "    \n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cv_results(cv, y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[ 88  13]\n",
      " [ 26 141]]\n",
      "Accuracy: 0.8544776119402985\n",
      "\n",
      "Best Parameters: {'clf__fit_intercept': True, 'features__text_pipeline__tfidf__use_idf': False, 'features__text_pipeline__vect__max_df': 0.5, 'features__text_pipeline__vect__max_features': None, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'char_counter': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "display_cv_results(cv, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true=y_test,\n",
    "                               y_pred=y_pred,\n",
    "                               target_names=['fake','true'],\n",
    "                               output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow0_col0 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow0_col1 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }    #T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow0_col2 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow1_col0 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }    #T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow1_col1 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow1_col2 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }</style><table id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcb\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >precision</th>        <th class=\"col_heading level0 col1\" >recall</th>        <th class=\"col_heading level0 col2\" >f1-score</th>        <th class=\"col_heading level0 col3\" >support</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcblevel0_row0\" class=\"row_heading level0 row0\" >fake</th>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow0_col0\" class=\"data row0 col0\" >0.771930</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow0_col1\" class=\"data row0 col1\" >0.871287</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow0_col2\" class=\"data row0 col2\" >0.818605</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow0_col3\" class=\"data row0 col3\" >101.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcblevel0_row1\" class=\"row_heading level0 row1\" >true</th>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow1_col0\" class=\"data row1 col0\" >0.915584</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow1_col1\" class=\"data row1 col1\" >0.844311</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow1_col2\" class=\"data row1 col2\" >0.878505</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow1_col3\" class=\"data row1 col3\" >167.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcblevel0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow2_col0\" class=\"data row2 col0\" >0.854478</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow2_col1\" class=\"data row2 col1\" >0.854478</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow2_col2\" class=\"data row2 col2\" >0.854478</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow2_col3\" class=\"data row2 col3\" >0.854478</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcblevel0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow3_col0\" class=\"data row3 col0\" >0.843757</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow3_col1\" class=\"data row3 col1\" >0.857799</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow3_col2\" class=\"data row3 col2\" >0.848555</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow3_col3\" class=\"data row3 col3\" >268.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcblevel0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow4_col0\" class=\"data row4 col0\" >0.861446</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow4_col1\" class=\"data row4 col1\" >0.854478</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow4_col2\" class=\"data row4 col2\" >0.855930</td>\n",
       "                        <td id=\"T_08c3735c_0c93_11eb_bf9e_38f9d3537bcbrow4_col3\" class=\"data row4 col3\" >268.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fec13205cd0>"
      ]
     },
     "execution_count": 1059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(report).transpose().style\\\n",
    ".highlight_max(color='lightgreen', subset=['precision'])\\\n",
    ".highlight_min(color='lightgreen', subset=['precision'] )\\\n",
    ".highlight_max(color='lightgreen', subset=['recall'])\\\n",
    ".highlight_min(color='lightgreen', subset=['recall'] )\\\n",
    ".highlight_max(color='lightgreen', subset=['f1-score'])\\\n",
    ".highlight_min(color='lightgreen', subset=['f1-score'] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
