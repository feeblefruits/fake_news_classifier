{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import string\n",
    "import re\n",
    "import bs4 as BeautifulSoup\n",
    "import fasttext\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/fakenews_corpus_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 130444 entries, 0 to 130443\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  130444 non-null  int64 \n",
      " 1   title       130443 non-null  object\n",
      " 2   label       130444 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Surprise: Socialist Hotbed Of Venezuela Has Lo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Water Cooler 1/25/18 Open Thread; Fake News ? ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Veteran Commentator Calls Out the Growing “Eth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lost Words, Hidden Words, Otters, Banks and Books</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Red Alert: Bond Yields Are SCREAMING “Inflatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  label\n",
       "0           0  Surprise: Socialist Hotbed Of Venezuela Has Lo...      0\n",
       "1           1  Water Cooler 1/25/18 Open Thread; Fake News ? ...      0\n",
       "2           2  Veteran Commentator Calls Out the Growing “Eth...      0\n",
       "3           3  Lost Words, Hidden Words, Otters, Banks and Books      0\n",
       "4           4  Red Alert: Bond Yields Are SCREAMING “Inflatio...      0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    123849\n",
       "1      6594\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising distribution of labels\n",
    "\n",
    "df = pd.concat([df[df['label'] == 0].head(6594), df[df['label'] == 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6594\n",
       "0    6594\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define pipeline settings</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity and just being neat a seperate library is imported, holding the custom transformers\n",
    "\n",
    "from custom_transformers import CharCounter\n",
    "from custom_transformers import CaseCounter\n",
    "from custom_transformers import StopWordCounter\n",
    "from custom_transformers import WordPronCounter\n",
    "from custom_transformers import WordNounCounter\n",
    "from custom_transformers import WordAdjCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    '''\n",
    "    INPUT: String to tokenise, detect and replace URLs\n",
    "    OUTPUT: List of tokenised string items\n",
    "    '''\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = [w for w in text.split() if not w in stop_words]\n",
    "\n",
    "    # Join list to string\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Replace URLs if any\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # Setup tokens and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Create tokens and lemmatize\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    \n",
    "    '''\n",
    "    INPUT: None\n",
    "    OUTPUT: pipeline object used to .fit X_train and y_train datasets\n",
    "    '''\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('char_counter', CharCounter()),\n",
    "            ('case_counter', CaseCounter()),\n",
    "            ('stop_counter', StopWordCounter()),\n",
    "            ('pro_counter', WordPronCounter()),\n",
    "            ('noun_counter', WordNounCounter()),\n",
    "            ('adj_counter', WordAdjCounter())\n",
    "        ])),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Accuracy results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred):\n",
    "    \n",
    "    '''\n",
    "    INPUT: y_test, y_pred dfs\n",
    "    OUTPUT: print average accuracy score\n",
    "    '''\n",
    "    \n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8507734303912647\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more granular performance evaluation, a classification report is initialised and displayed as a pd table\n",
    "\n",
    "report = classification_report(y_true=y_test,\n",
    "                               y_pred=y_pred,\n",
    "                               target_names=['fake','true'],\n",
    "                               output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow0_col0 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow0_col1 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }    #T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow0_col2 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }    #T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow1_col0 {\n",
       "            background-color:  lightgreen;\n",
       "            : ;\n",
       "        }    #T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow1_col1 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow1_col2 {\n",
       "            : ;\n",
       "            background-color:  lightgreen;\n",
       "        }</style><table id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcb\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >precision</th>        <th class=\"col_heading level0 col1\" >recall</th>        <th class=\"col_heading level0 col2\" >f1-score</th>        <th class=\"col_heading level0 col3\" >support</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcblevel0_row0\" class=\"row_heading level0 row0\" >fake</th>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow0_col0\" class=\"data row0 col0\" >0.823428</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow0_col1\" class=\"data row0 col1\" >0.884687</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow0_col2\" class=\"data row0 col2\" >0.852959</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow0_col3\" class=\"data row0 col3\" >1613.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcblevel0_row1\" class=\"row_heading level0 row1\" >true</th>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow1_col0\" class=\"data row1 col0\" >0.881074</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow1_col1\" class=\"data row1 col1\" >0.818290</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow1_col2\" class=\"data row1 col2\" >0.848522</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow1_col3\" class=\"data row1 col3\" >1684.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcblevel0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow2_col0\" class=\"data row2 col0\" >0.850773</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow2_col1\" class=\"data row2 col1\" >0.850773</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow2_col2\" class=\"data row2 col2\" >0.850773</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow2_col3\" class=\"data row2 col3\" >0.850773</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcblevel0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow3_col0\" class=\"data row3 col0\" >0.852251</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow3_col1\" class=\"data row3 col1\" >0.851488</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow3_col2\" class=\"data row3 col2\" >0.850740</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow3_col3\" class=\"data row3 col3\" >3297.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcblevel0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow4_col0\" class=\"data row4 col0\" >0.852872</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow4_col1\" class=\"data row4 col1\" >0.850773</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow4_col2\" class=\"data row4 col2\" >0.850693</td>\n",
       "                        <td id=\"T_f60c2b96_0c51_11eb_a1cc_38f9d3537bcbrow4_col3\" class=\"data row4 col3\" >3297.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa3dfbd7b10>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(report).transpose().style\\\n",
    ".highlight_max(color='lightgreen', subset=['precision'])\\\n",
    ".highlight_min(color='lightgreen', subset=['precision'] )\\\n",
    ".highlight_max(color='lightgreen', subset=['recall'])\\\n",
    ".highlight_min(color='lightgreen', subset=['recall'] )\\\n",
    ".highlight_max(color='lightgreen', subset=['f1-score'])\\\n",
    ".highlight_min(color='lightgreen', subset=['f1-score'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple classifier evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  153.26657629013062\n",
      "fit_time  std  2.336130197826069\n",
      "score_time  mean  73.92878603935242\n",
      "score_time  std  4.408559949784302\n",
      "test_score  mean  0.8328773216297352\n",
      "test_score  std  0.006350048555057398\n",
      "---------------------------------\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "-----------------------------------\n",
      "fit_time  mean  153.82260012626648\n",
      "fit_time  std  17.072552543485408\n",
      "score_time  mean  75.17530083656311\n",
      "score_time  std  9.508021235384769\n",
      "test_score  mean  0.7647343162707045\n",
      "test_score  std  0.010305675464602777\n",
      "---------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  190.5718227227529\n",
      "fit_time  std  25.734956107861475\n",
      "score_time  mean  115.88444089889526\n",
      "score_time  std  37.17358487850151\n",
      "test_score  mean  0.7799001708460427\n",
      "test_score  std  0.006980199479111797\n",
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  212.2000022729238\n",
      "fit_time  std  8.69944611535265\n",
      "score_time  mean  134.4990242322286\n",
      "score_time  std  65.09193669904604\n",
      "test_score  mean  0.5712270773188516\n",
      "test_score  std  0.006525985146591894\n",
      "---------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  176.38443128267923\n",
      "fit_time  std  15.091482134602323\n",
      "score_time  mean  78.88867131868999\n",
      "score_time  std  8.198577101635493\n",
      "test_score  mean  0.8492574124465055\n",
      "test_score  std  0.004570144392231772\n",
      "---------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "-----------------------------------\n",
      "fit_time  mean  252.91105071703592\n",
      "fit_time  std  126.85006978611817\n",
      "score_time  mean  145.4233009815216\n",
      "score_time  std  44.29679747044982\n",
      "test_score  mean  0.630371022061779\n",
      "test_score  std  0.008007362386555066\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "-----------------------------------\n",
      "fit_time  mean  370.2380406856537\n",
      "fit_time  std  28.13397081995182\n",
      "score_time  mean  157.29323196411133\n",
      "score_time  std  38.14035381241317\n",
      "test_score  mean  0.7948629939580281\n",
      "test_score  std  0.01633052355989663\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(AdaBoostClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "clfs.append(SVC())\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "\n",
    "classifier_name = []\n",
    "mean_value = []\n",
    "std_value = []\n",
    "\n",
    "for classifier in clfs:\n",
    "    model.set_params(clf = classifier)\n",
    "    scores = cross_validate(model, X_train, y_train)\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    for key, values in scores.items():\n",
    "        \n",
    "        classifier_name.append(classifier)\n",
    "        mean_value.append(values.mean())\n",
    "        std_value.append(values.std())\n",
    "        \n",
    "        print(key,' mean ', values.mean())\n",
    "        print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
